const { GoogleGenAI, FunctionCallingConfigMode } = require('@google/genai');
const { Readable } = require('stream');
const { HomeyMCPAdapter } = require('./HomeyMCPAdapter');

// Gemini model configuration
const GEMINI_MODEL = "gemini-2.5-flash-lite";           // Main model for text/multimodal
const GEMINI_IMAGE_MODEL = "gemini-2.5-flash-image";   // Model for image generation

// System instruction for smart home assistant (following Google's function calling best practices)
const SYSTEM_INSTRUCTION = `You are a helpful smart home assistant that helps users control their Homey devices and flows.

Your role:
- Control devices (lights, thermostats, switches, etc.)
- Query device status and information
- Trigger automation flows
- Answer questions about the home state

Guidelines:
- Don't guess device names. If a device is not found, use list_devices_in_zone or list_all_devices to find the exact name.
- When users mention room names (e.g., "studio", "camera"), use list_devices_in_zone with the zone name first.
- For queries like "which lights are on?", use get_devices_status_by_class with deviceClass="light".
- For zone-specific queries like "lights on in kitchen?", use get_devices_status_by_class with both deviceClass and zone parameters.
- Always provide clear, natural language responses after executing functions.
- If a function call fails, try alternative approaches before giving up.
- Be concise but friendly in your responses.`;

class GeminiClient {
  constructor(apiKey, options = {}) {
    if (!apiKey) {
      throw new Error("API key is required");
    }
    this.genAI = new GoogleGenAI({ apiKey: apiKey });
    this.options = options;
    this.homey = options.homey;
    this.mcpAdapter = this.homey ? new HomeyMCPAdapter(this.homey) : null;
    this.mcpModel = null;
  }

  async generateText(prompt) {
    console.log("Generating text with prompt:", prompt);

    const response = await this.genAI.models.generateContent({
      model: GEMINI_MODEL,
      contents: prompt
    });
    return response.text;
  }

  /**
   * Generates text from a multimodal prompt (text + image).
   * @param {string} textPrompt The text prompt to send.
   * @param {Buffer} imageBuffer The image as a Buffer.
   * @param {string} mimeType The MIME type of the image (e.g., 'image/jpeg', 'image/png').
   * @returns {Promise<string>} The generated text response.
   */
  async generateTextWithImage(textPrompt, imageBuffer, mimeType = 'image/jpeg') {
    console.log("Generating text with image, prompt:", textPrompt);
    console.log("Image buffer size:", imageBuffer.length, "bytes, mimeType:", mimeType);

    // Convert Buffer to Base64
    const base64Image = imageBuffer.toString('base64');

    // Build multimodal content array
    // Tip from Gemini docs: place text prompt after the image for best results with single image
    const contents = [
      {
        inlineData: {
          mimeType: mimeType,
          data: base64Image,
        },
      },
      textPrompt,
    ];

    const response = await this.genAI.models.generateContent({
      model: GEMINI_MODEL,
      contents: contents,
    });

    return response.text;
  }

  /**
   * Helper to convert a readable stream to a Buffer.
   * @param {import('stream').Readable} stream The readable stream.
   * @returns {Promise<Buffer>} The buffer containing the stream data.
   */
  static async streamToBuffer(stream) {
    const chunks = [];
    return new Promise((resolve, reject) => {
      stream.on('data', (chunk) => chunks.push(chunk));
      stream.on('end', () => resolve(Buffer.concat(chunks)));
      stream.on('error', (err) => reject(err));
    });
  }

  /**
  * @param {string} prompt The description of the image to generate.
  * @returns {string} The Base64 string of the generated image (the token).
  */
  async generateImage(prompt) {
    console.log("Generating image with Gemini (text-to-image) for prompt:", prompt);

    try {
      console.log("Calling Gemini image generation API...");
      
      // Use image generation model for text-to-image generation (requires paid tier)
      const response = await this.genAI.models.generateContent({
        model: GEMINI_IMAGE_MODEL,
        contents: prompt,
      });

      console.log("Gemini API response received");

      // Extract the image from the response
      // The response contains parts which can be text or inlineData (images)
      if (!response.candidates || response.candidates.length === 0) {
        throw new Error("No candidates in Gemini response");
      }

      const parts = response.candidates[0].content.parts;
      
      // Find the image part (inlineData)
      const imagePart = parts.find(part => part.inlineData);
      
      if (!imagePart) {
        throw new Error("No image generated by Gemini");
      }

      const base64ImageBytes = imagePart.inlineData.data;

      if (!base64ImageBytes) {
        throw new Error("No image data in response");
      }

      console.log("Image generated successfully, bytes length:", base64ImageBytes.length);

      // Return the Base64 string
      return base64ImageBytes;
    } catch (error) {
      console.error("Error generating image with Gemini:", error);
      throw error;
    }
  }

  /**
   * Generates an image and returns it as a Readable stream.
   * @param {string} prompt The description of the image to generate.
   * @returns {Promise<Readable>} A Readable stream of the generated image.
   */
  async generateImageStream(prompt) {
    console.log("generateImageStream called");
    const base64Image = await this.generateImage(prompt);
    console.log("Converting base64 to buffer...");
    const imageBuffer = Buffer.from(base64Image, 'base64');
    console.log("Buffer created, size:", imageBuffer.length, "bytes");
    const stream = Readable.from(imageBuffer);
    console.log("Stream created");
    return stream;
  }

  /**
   * Generate text with MCP function calling support (multi-turn loop)
   * Uses ANY mode to force function calling until an action succeeds,
   * then switches to NONE (no tools) to allow final text response.
   * @param {string} prompt - User command/prompt
   * @returns {Promise<{response: string, success: boolean}>} - Response text and success status
   */
  async generateTextWithMCP(prompt) {
    if (!this.mcpAdapter) {
      throw new Error("MCP Adapter not available. Homey instance is required.");
    }

    console.log(`[MCP] User command: "${prompt}"`);

    // Get tools from MCP adapter
    const toolsList = await this.mcpAdapter.listTools();
    console.log(`[MCP] Available tools: ${toolsList.tools.length}`);
    const functionDeclarations = toolsList.tools.map(tool => ({
      name: tool.name,
      description: tool.description,
      parameters: tool.inputSchema
    }));

    // Build conversation history
    const contents = [
      { role: "user", parts: [{ text: prompt }] }
    ];

    // Track different types of successes
    let hasDeviceAction = false;      // control_device or trigger_flow succeeded
    let hasInformativeAction = false; // informative query succeeded
    
    // Dispositive actions that modify device states or trigger automations
    const dispositiveActions = ['control_device', 'trigger_flow'];
    
    // Informative actions that retrieve information to answer the user's question
    // These trigger AUTO mode because they provide the final answer
    const informativeActions = [
      // 'get_lights_status',  // Commented out - use get_devices_status_by_class instead
      'get_devices_status_by_class',
      'get_device_count_by_zone'
    ];
    
    // Discovery/support tools that help find information but don't answer the question directly
    // These do NOT trigger AUTO mode - they're intermediate steps
    // Examples: get_capability_info, get_device_class_info, list_zones, list_devices, list_devices_in_zone

    const MAX_TURNS = 10;
    let turnCount = 0;

    while (turnCount < MAX_TURNS) {
      turnCount++;
      
      // Determine mode based on action type:
      // - If device action succeeded → NONE (force text response)
      // - If only informative action succeeded and 2+ turns passed → NONE (force text response)
      // - Otherwise → ANY (keep searching for actions)
      const shouldSwitchToNone = hasDeviceAction || 
                                  (hasInformativeAction && turnCount >= 2);
      
      const callingMode = shouldSwitchToNone
        ? FunctionCallingConfigMode.NONE 
        : FunctionCallingConfigMode.ANY;
      
      console.log(`[MCP] Turn ${turnCount}: mode=${shouldSwitchToNone ? 'NONE' : 'ANY'}, hasDeviceAction=${hasDeviceAction}, hasInformativeAction=${hasInformativeAction}`);

      // Call API with conversation history
      // In NONE mode, remove tools config to force text-only response
      const apiConfig = shouldSwitchToNone
        ? {
            systemInstruction: SYSTEM_INSTRUCTION,
            temperature: 0  // Low temperature for more deterministic responses
          }
        : {
            systemInstruction: SYSTEM_INSTRUCTION,
            temperature: 0,  // Low temperature for more deterministic function calling
            tools: [{ functionDeclarations: functionDeclarations }],
            toolConfig: {
              functionCallingConfig: {
                mode: callingMode
              }
            }
          };

      console.log(`[MCP] Calling generateContent with mode=${callingMode}, history_length=${contents.length}`);

      const response = await this.genAI.models.generateContent({
        model: GEMINI_MODEL,
        contents: contents,
        config: apiConfig
      });

      const candidate = response.candidates?.[0];
      if (!candidate?.content?.parts) {
        console.log("[MCP] ERROR: No valid response from API");
        // If we switched to AUTO but got no response, there might be an issue
        // Log the response for debugging
        console.log("[MCP] Response object:", JSON.stringify(response, null, 2));
        
        if (hasInformativeAction || hasDeviceAction) {
          return {
            response: "I retrieved the information but couldn't formulate a response. Please try rephrasing your question.",
            success: false
          };
        }
        return {
          response: response.text || "No response generated.",
          success: false
        };
      }

      // Validate finishReason (following Google's best practices)
      const finishReason = candidate.finishReason;
      console.log(`[MCP] finishReason: ${finishReason}`);
      if (finishReason === "SAFETY") {
        console.log("[MCP] ERROR: Response blocked by safety filters");
        return {
          response: "I cannot process this request due to safety guidelines.",
          success: false
        };
      }
      
      if (finishReason === "MAX_TOKENS") {
        console.log("[MCP] ERROR: Response truncated due to token limit");
        return {
          response: "The response was too long. Please try a simpler query.",
          success: false
        };
      }
      
      if (finishReason !== "STOP" && finishReason !== undefined) {
        console.warn(`[MCP] WARN: Unexpected finishReason: ${finishReason}`);
      }

      // Add model response to history
      contents.push(candidate.content);

      // Check for function calls
      const functionCalls = candidate.content.parts.filter(p => p.functionCall);

      if (functionCalls.length === 0) {
        // No function calls - return final text
        if (shouldSwitchToNone) {
          console.log(`[MCP] Final response generated after ${turnCount} turn(s)`);
          const finalResponse = response.text || "Operation completed successfully.";
          return {
            response: finalResponse,
            // Success if we got a meaningful response (not empty/default)
            success: response.text && response.text.length > 0
          };
        } else {
          // Should not happen with ANY mode, but handle gracefully
          console.log(`[MCP] WARN: No function calls in ANY mode, turn=${turnCount}`);
          return {
            response: response.text || "Unable to complete the requested action.",
            success: false
          };
        }
      }

      // Execute function calls
      // Following Google's best practice: execute multiple independent function calls in parallel
      console.log(`[MCP] Executing ${functionCalls.length} function(s):`);
      if (functionCalls.length > 1) {
        console.log(`[MCP] Parallel execution: ${functionCalls.length} functions`);
      }

      // Execute all function calls in parallel using Promise.all
      const functionPromises = functionCalls.map(async (part) => {
        const call = part.functionCall;
        console.log(`[MCP]   calling ${call.name}(${JSON.stringify(call.args)})`);

        try {
          const result = await this.mcpAdapter.callTool(call.name, call.args);
          const resultStr = JSON.stringify(result);
          console.log(`[MCP]   ${result.success ? 'OK' : 'FAIL'} ${call.name}: ${resultStr}`);

          return {
            call,
            result,
            success: result.success === true
          };
        } catch (error) {
          console.error(`[MCP]   FAIL ${call.name}: ${error.message}`);
          return {
            call,
            result: { success: false, error: error.message },
            success: false
          };
        }
      });

      // Wait for all function calls to complete
      const executedFunctions = await Promise.all(functionPromises);

      // Process results and build response array
      const functionResponses = [];
      for (const { call, result, success } of executedFunctions) {
        // Track different types of successes
        if (success) {
          if (dispositiveActions.includes(call.name)) {
            hasDeviceAction = true;
            console.log(`[MCP]   SUCCESS: Device action ${call.name}`);
          } else if (informativeActions.includes(call.name)) {
            hasInformativeAction = true;
            console.log(`[MCP]   SUCCESS: Informative action ${call.name}`);
          }
        }

        functionResponses.push({
          functionResponse: {
            name: call.name,
            response: result
          }
        });
      }

      // Add function responses to history
      contents.push({
        role: "function",
        parts: functionResponses
      });
      console.log(`[MCP] Added ${functionResponses.length} function result(s) to history for next turn`);

      // Continue loop to let model see results
    }

    console.log(`[MCP] ERROR: Max turns (${MAX_TURNS}) reached`);
    return {
      response: "Operation partially completed. Maximum interaction limit reached.",
      success: false
    };
  }

  /* ============================================================================
   * NOTE: Automatic Function Calling (AFC) 
   * ============================================================================
   * AFC is a Python SDK-only feature that is NOT available in JavaScript/Node.js.
   * 
   * From the official documentation:
   * "Automatic function calling is a Python SDK feature only."
   * 
   * The manual loop implementation in generateTextWithMCP() is the correct and
   * only approach for JavaScript, following Google's documented patterns for
   * multi-turn function calling in non-Python SDKs.
   * 
   * Reference: https://ai.google.dev/gemini-api/docs/function-calling
   * ============================================================================
   */


}

module.exports = { GeminiClient };
